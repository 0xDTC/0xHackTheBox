#!/bin/bash

echo "Fetching Hack The Box machine page..."
curl -s "https://www.hackthebox.com/machines" -A "Mozilla/5.0" > debug_full_page.html

# Extract the relevant section while skipping #not-found
awk '/id="machines"/ {flag=1; next} /id="not-found"/ {next} flag' debug_full_page.html > debug_machine_section.html

# Debugging logs
echo "Saved full page content to debug_full_page.html"
echo "Saved extracted machine section to debug_machine_section.html"

# Extract machine about links (unique machine URLs)
MACHINE_ABOUT_LINKS=($(grep -oP '(?<=href=")https://www.hackthebox.com/machines/[^"]+' debug_machine_section.html))
MACHINE_LINKS=($(echo "${MACHINE_ABOUT_LINKS[@]}" | sed 's|www|app|g'))  # Convert to app.hackthebox.com
IMAGE_LINKS=($(grep -oP '(?<=src=")https://htb-mp-prod-public-storage\.s3\.eu-central-1\.amazonaws\.com/avatars/[^"]+' debug_machine_section.html))

# Check extracted counts
echo -e "\nExtracted ${#MACHINE_ABOUT_LINKS[@]} machine about links, ${#MACHINE_LINKS[@]} machine links, and ${#IMAGE_LINKS[@]} images.\n"

# Define output file
OUTPUT_FILE="htb_machines.txt"

# Write header to file
echo -e "Machine Name | Machine About URL | Machine URL | Machine Image URL" > "$OUTPUT_FILE"
echo -e "------------|-------------------|------------|-------------------" >> "$OUTPUT_FILE"

# Create a directory for images
mkdir -p htb_machine_images

# Loop through extracted machine about pages to fetch machine names
for i in "${!MACHINE_ABOUT_LINKS[@]}"; do
    ABOUT_PAGE="${MACHINE_ABOUT_LINKS[$i]}"
    echo "Fetching machine page: $ABOUT_PAGE"
    
    # Fetch machine page content
    curl -s "$ABOUT_PAGE" -A "Mozilla/5.0" > machine_page.html

    # Extract machine name using regex (Find first <h1> title in page)
    MACHINE_NAME=$(grep -oP '(?<=<h1>).*?(?=</h1>)' machine_page.html | head -n 1)

    # If no name is found, fallback to extracting from the URL last segment
    if [[ -z "$MACHINE_NAME" ]]; then
        MACHINE_NAME=$(basename "$ABOUT_PAGE")
    fi

    # Store the machine name in an array for later renaming
    MACHINE_NAMES[$i]="$MACHINE_NAME"

    # Append extracted details to the file
    echo -e "${MACHINE_NAME} | ${MACHINE_ABOUT_LINKS[$i]} | ${MACHINE_LINKS[$i]} | ${IMAGE_LINKS[$i]}" >> "$OUTPUT_FILE"

    # Cleanup temp machine page file
    rm -f machine_page.html
done

echo "Saved extracted machine details to $OUTPUT_FILE"

# Download images 20 at a time
echo "Downloading machine images..."
DOWNLOADED_COUNT=0
SKIPPED_COUNT=0

for ((j = 0; j < ${#IMAGE_LINKS[@]}; j += 20)); do
    BATCH=("${IMAGE_LINKS[@]:j:20}")
    BATCH_DOWNLOAD_COUNT=0

    for k in "${!BATCH[@]}"; do
        INDEX=$((j + k))
        IMAGE_URL="${BATCH[$k]}"
        MACHINE_NAME="${MACHINE_NAMES[$INDEX]}"

        # Download and rename image only if it doesn't exist
        if [[ ! -f "htb_machine_images/${MACHINE_NAME}.png" ]]; then
            wget -q "$IMAGE_URL" -O "htb_machine_images/${MACHINE_NAME}.png" &
            ((BATCH_DOWNLOAD_COUNT++))
            ((DOWNLOADED_COUNT++))
        else
            ((SKIPPED_COUNT++))
        fi
    done

    # Only wait and sleep if we actually downloaded something
    if [[ $BATCH_DOWNLOAD_COUNT -gt 0 ]]; then
        wait  # Wait for all downloads to complete
        echo "Downloaded $BATCH_DOWNLOAD_COUNT images in this batch (Total: $DOWNLOADED_COUNT downloaded, $SKIPPED_COUNT skipped), waiting for 10 seconds..."
        sleep 10
    fi
done

echo "Download complete! Downloaded: $DOWNLOADED_COUNT, Skipped: $SKIPPED_COUNT"

echo "All images downloaded and renamed."

# Remove temporary files
rm -f debug_full_page.html debug_machine_section.html

echo "Cleanup completed. Temporary files removed."
